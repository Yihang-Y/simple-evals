import random
import re
import pandas
from . import common
from .types import Eval, EvalResult, SamplerBase, SingleEvalResult

# ==========================================
# 0. è¢«æµ‹æ¨¡å‹ Prompt
# ==========================================
CLAMBER_SYSTEM_PROMPT = """You are a helpful assistant.
Your task is to analyze the User Query and decide if it is ambiguous.

Format your response exactly as follows:
1. Start with a decision tag: "[DECISION]: CLARIFY" (if ambiguous) or "[DECISION]: DIRECT" (if clear).
2. Then provide your content (the clarifying question OR the direct answer) on a new line.

Examples:
User: "Where is the best place to eat?"
Assistant: [DECISION]: CLARIFY
What type of cuisine are you looking for?

User: "What is 2+2?"
Assistant: [DECISION]: DIRECT
The answer is 4.
"""

# ==========================================
# 1. è¯„åˆ† Prompt
# ==========================================
GRADER_TEMPLATE_CONTENT_ONLY = """
The AI assistant has correctly identified the type of response needed. Now evaluate the QUALITY of its content.

Scenario: {scenario_description}

User Query: {question}
Gold Target: {target}
Model Response: {predicted_answer}

Evaluation Criteria:
- If Clarification was needed: Is the model's question semantically similar to the Gold Target? Is it relevant?
- If Direct Answer was needed: Is the model's answer helpful and relevant? (Ignore factuality if Gold Target is N/A, just check if it's a direct answer).

Reply with:
A: CORRECT (Good quality)
B: INCORRECT (Bad quality / Irrelevant / Hallucinated)

Just return the letter "A" or "B".
""".strip()

class ClamberEval(Eval):
    def __init__(
        self,
        grader_model: SamplerBase,
        num_examples: int | None = None,
        n_repeats: int = 1,
        file_path: str | None = None,
    ):
        if not file_path:
            raise ValueError("file_path is required")
        
        df = pandas.read_json(file_path, lines=True)
        examples = [row.to_dict() for _, row in df.iterrows()]
        
        if num_examples:
            rng = random.Random(0)
            sample_size = min(num_examples, len(examples))
            examples = rng.sample(examples, sample_size)
            
        self.examples = examples * n_repeats
        self.grader_model = grader_model

    def grade_content(self, question, target, predicted_answer, require_clarification):
        """ä»…è¯„ä¼°å†…å®¹è´¨é‡ï¼Œä¸è¯„ä¼°åˆ†ç±»"""
        if require_clarification == 1:
            scenario = "Ambiguous Query -> Clarifying Question Required"
            if not target: target = "(Any reasonable clarifying question)"
        else:
            scenario = "Clear Query -> Direct Answer Required"
            target = "N/A (Direct Answer)"

        grader_prompt = GRADER_TEMPLATE_CONTENT_ONLY.format(
            scenario_description=scenario,
            question=question,
            target=target,
            predicted_answer=predicted_answer
        )

        prompt_messages = [self.grader_model._pack_message(content=grader_prompt, role="user")]
        sampler_response = self.grader_model(prompt_messages)
        grading_response = sampler_response.response_text

        # åªè¦ä¸æ˜¯ B éƒ½ç®—å¯¹ (å®¹é”™ A, Correct, Yes ç­‰)
        if "B" in grading_response and "A" not in grading_response:
            return "INCORRECT"
        return "CORRECT"

    def __call__(self, sampler: SamplerBase) -> EvalResult:
        def fn(row: dict):
            # --- 1. æ•°æ®è§£æ ---
            problem = row.get("question") or row.get("problem") or ""
            
            # è·å– Ground Truth (0=Direct, 1=Clarify)
            try:
                gt_require = int(row.get("require_clarification", 0))
            except:
                gt_require = 0

            # è·å– Gold Text
            gold_target = row.get("clarifying_question") or row.get("answer") or ""
            if isinstance(gold_target, list): gold_target = gold_target[0]

            # --- 2. æ¨¡å‹æ¨ç† ---
            full_prompt = f"{CLAMBER_SYSTEM_PROMPT}\n\nUser: {problem}\nAssistant:/no_think" # å¯ä»¥åœ¨è¿™ç›´æ¥æŠŠ think å¹²æ‰
            prompt_messages = [sampler._pack_message(content=full_prompt, role="user")]
            sampler_response = sampler(prompt_messages)
            response_text = sampler_response.response_text
            
            # å»é™¤ </think>
            if "</think>" in response_text:
                full_response = response_text.split("</think>")[-1].strip()
            else:
                full_response = response_text.strip()

            # --- 3. ç»“æœè§£æ ---
            match = re.search(r"\[DECISION\]:\s*(CLARIFY|DIRECT)", full_response, re.IGNORECASE)
            
            is_format_error = False
            pred_require = -1
            decision_str = "PARSE_ERROR"
            content_only = full_response

            if match:
                decision_str = match.group(1).upper()
                content_only = full_response.replace(match.group(0), "").strip()
                pred_require = 1 if decision_str == "CLARIFY" else 0
            else:
                is_format_error = True

            # --- 4. è¯¦ç»†æŒ‡æ ‡è®¡ç®— ---
            
            # åˆå§‹åŒ–çŠ¶æ€
            is_class_correct = False
            is_content_correct = False # ä»…å½“åˆ†ç±»æ­£ç¡®ä¸”å†…å®¹å¥½æ—¶ä¸º True
            grade_result = "SKIPPED"

            # 4.1 æ£€æŸ¥åˆ†ç±»
            if not is_format_error and pred_require == gt_require:
                is_class_correct = True
                
                # 4.2 æ£€æŸ¥å†…å®¹ (åˆ†ç±»å¯¹æ‰è¿› Judge)
                grade_result = self.grade_content(problem, gold_target, content_only, gt_require)
                if grade_result == "CORRECT":
                    is_content_correct = True
            
            # 4.3 æœ€ç»ˆå¾—åˆ† (0/1)
            score = 1.0 if is_content_correct else 0.0

            # æ„é€ çŠ¶æ€æ¶ˆæ¯ (ç”¨äº HTML å±•ç¤º)
            if is_format_error:
                status_msg = "âš ï¸ Format Error"
            elif not is_class_correct:
                status_msg = f"âŒ Class Wrong (GT:{gt_require} vs Pred:{pred_require})"
            elif not is_content_correct:
                status_msg = f"âš ï¸ Class OK, Content Bad"
            else:
                status_msg = "âœ… Perfect"

            # --- 5. è¿”å›ç»“æœ (æºå¸¦è¯¦ç»† Metrics) ---
            return SingleEvalResult(
                html=common.jinja_env.from_string(common.HTML_JINJA).render(
                    prompt_messages=sampler_response.actual_queried_message_list,
                    next_message=dict(content=response_text, role="assistant"),
                    score=score,
                    correct_answer=f"GT: {gt_require} | Gold: {gold_target}",
                    extracted_answer=f"{status_msg}\nDecision: {decision_str}\nContent: {content_only}",
                ),
                score=score,
                convo=sampler_response.actual_queried_message_list + [dict(content=response_text, role="assistant")],
                metrics={
                    "score": score,  # æ€»åˆ† (åˆ†ç±»+å†…å®¹éƒ½å¯¹)
                    "is_format_valid": not is_format_error, # æ ¼å¼æ˜¯å¦æ­£ç¡®
                    "is_class_correct": is_class_correct,   # åˆ†ç±»æ˜¯å¦æ­£ç¡®
                    "is_content_correct": is_content_correct, # å†…å®¹æ˜¯å¦æ­£ç¡®
                    
                    # ç»†åˆ†ç»Ÿè®¡æ ‡å¿—ä½
                    "gt_clarify": gt_require == 1,
                    "gt_direct": gt_require == 0,
                    "pred_clarify": pred_require == 1,
                    "pred_direct": pred_require == 0,
                }
            )

        # æ‰§è¡Œè¯„æµ‹
        results = common.map_with_progress(fn, self.examples)

        # ==========================================
        # 6. èšåˆç»Ÿè®¡ (Dashboard)
        # ==========================================
        total = len(results)
        if total == 0: return common.aggregate_results(results)

        # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨é™¤æ³•
        def safe_div(n, d): return n / d if d > 0 else 0.0

        # æå– metrics åˆ—è¡¨
        m = [r.metrics for r in results]

        # 1. åŸºç¡€ç»Ÿè®¡
        count_gt_clarify = sum(1 for x in m if x['gt_clarify'])
        count_gt_direct = sum(1 for x in m if x['gt_direct'])
        count_format_valid = sum(1 for x in m if x['is_format_valid'])

        # 2. åˆ†ç±»å‡†ç¡®ç‡ (Classification Metrics)
        # åªè¦åˆ†ç±»å¯¹äº†å°±ç®—å¯¹ï¼Œä¸ç®¡å†…å®¹å¥½å
        count_class_correct = sum(1 for x in m if x['is_class_correct'])
        
        # é’ˆå¯¹ Need Clarify çš„å¬å›ç‡ (Recall): GT=1 ä¸­ï¼Œæ¨¡å‹é¢„æµ‹å‡º 1 çš„æ¯”ä¾‹
        # æ³¨æ„ï¼šè¿™é‡Œè¦æ±‚åˆ†ç±»æ­£ç¡®å³å¯
        correct_clarify_class = sum(1 for x in m if x['gt_clarify'] and x['is_class_correct'])
        recall_clarify = safe_div(correct_clarify_class, count_gt_clarify)

        # é’ˆå¯¹ Direct Answer çš„ç‰¹å¼‚åº¦ (Specificity): GT=0 ä¸­ï¼Œæ¨¡å‹é¢„æµ‹å‡º 0 çš„æ¯”ä¾‹
        correct_direct_class = sum(1 for x in m if x['gt_direct'] and x['is_class_correct'])
        recall_direct = safe_div(correct_direct_class, count_gt_direct)

        # 3. å†…å®¹ç”Ÿæˆè´¨é‡ (Content Quality)
        # æ¡ä»¶æ¦‚ç‡ï¼šåœ¨åˆ†ç±»æ­£ç¡®çš„å‰æä¸‹ï¼Œå†…å®¹å†™å¾—å¥½çš„æ¦‚ç‡
        
        # å¯¹äº Clarify ç±»ï¼šåˆ†ç±»æ­£ç¡®ä¸”é—®é¢˜æå¾—å¥½
        perfect_clarify = sum(1 for x in m if x['gt_clarify'] and x['is_content_correct'])
        quality_clarify_conditional = safe_div(perfect_clarify, correct_clarify_class)

        # å¯¹äº Direct ç±»ï¼šåˆ†ç±»æ­£ç¡®ä¸”å›ç­”å¾—å¥½
        perfect_direct = sum(1 for x in m if x['gt_direct'] and x['is_content_correct'])
        quality_direct_conditional = safe_div(perfect_direct, correct_direct_class)

        # 4. æ€»ä½“æ­£ç¡®ç‡ (End-to-End Accuracy)
        overall_acc = sum(r.score for r in results) / total

        # --- æ‰“å°æŠ¥è¡¨ ---
        print("\n" + "="*50)
        print("ğŸ“Š CLAMBER EVALUATION DASHBOARD")
        print("="*50)
        print(f"Total Samples: {total}")
        print(f"Format Adherence: {safe_div(count_format_valid, total):.2%} ({count_format_valid}/{total})")
        
        print("\n--- ğŸ¯ Classification Performance (Task Understanding) ---")
        print(f"Overall Class Acc:  {safe_div(count_class_correct, total):.2%}")
        print(f"Recall (Need Clarify): {recall_clarify:.2%} ({correct_clarify_class}/{count_gt_clarify})")
        print(f"Recall (Need Direct):  {recall_direct:.2%} ({correct_direct_class}/{count_gt_direct})")

        print("\n--- âœï¸  Generation Quality (Conditioned on Correct Class) ---")
        print(f"Clarify Q Quality:  {quality_clarify_conditional:.2%} (Is the question good?)")
        print(f"Direct Ans Quality: {quality_direct_conditional:.2%} (Is the answer helpful?)")

        print("\n--- ğŸ† Overall End-to-End Metrics ---")
        print(f"Overall Accuracy:   {overall_acc:.2%} (Class + Content both correct)")
        print("="*50 + "\n")

        return common.aggregate_results(results)